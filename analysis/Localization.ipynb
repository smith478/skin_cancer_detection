{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization\n",
    "\n",
    "One crucial step in the modeling process is identifying the *reason* that a model is making its predictions. For convolutional neural networks the standard tool for this are gradient based class activation maps. Keras has great example code which we use here. See [this](https://keras.io/examples/vision/grad_cam/) and [this](https://keras.io/examples/vision/integrated_gradients/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- Go back to integrated gradients and make it work with our data\n",
    "- Look at other model interpretability methods\n",
    "- Give a description or resource on Grad CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dictionary of images for our dataset and create a lookup table for readable names for our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('..', 'data')\n",
    "\n",
    "# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\n",
    "\n",
    "image_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(base_dir, '*', '*.jpg'))}\n",
    "\n",
    "# This dictionary is useful for displaying more human-friendly labels later on\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis-like lesions',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(image_path_dict)} images in our dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read and process the data. This will help later with creating labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df = pd.read_csv(os.path.join(base_dir, 'datasets_54339_104884_HAM10000_metadata.csv'))\n",
    "\n",
    "# Creating New Columns for better readability\n",
    "\n",
    "skin_df['path'] = skin_df['image_id'].map(image_path_dict.get)\n",
    "skin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get) \n",
    "skin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "Look for duplicate images from patients and make sure datasets are stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = skin_df.groupby('lesion_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='image_id', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original dataset had {skin_df.shape[0]} records, there are {df.shape[0]} unique lesions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Test, and Val Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO Use TF Records and tf dataset to store training dataset as an alternative to data generator below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are numerous images taken for some patients, therefore we will choose a single image from each patient. Then we will take a stratified sample across our target variable in order to create our train test and validation directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dataframe containing a single image from each patient. Note that we could also try including these duplicates, just making sure that when we split our dataset we keep patients in a single train, test, or val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed (random_state) for reproducibility and deterministic train/val/test sets\n",
    "df_dataset = skin_df.sample(frac=1, random_state=123).drop_duplicates(subset='lesion_id').copy()\n",
    "df_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    'nv' ,\n",
    "    'mel', \n",
    "    'bkl', \n",
    "    'bcc',\n",
    "    'akiec',\n",
    "    'vasc',\n",
    "    'df',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use image generator to build model, then below use the newer tf records to orchastrate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "\n",
    "fig, m_axs = plt.subplots(7, n_samples, figsize = (4*n_samples, 3*7))\n",
    "for n_axs, (type_name, type_rows) in zip(m_axs, df_dataset.sort_values(['cell_type']).groupby('cell_type')):\n",
    "    n_axs[0].set_title(type_name)\n",
    "    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=1234).iterrows()):\n",
    "        c_ax.imshow(np.asarray(Image.open(c_row['path'])))\n",
    "        c_ax.axis('off')\n",
    "# The line below will save the images to disk\n",
    "#fig.savefig('category_samples.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = np.asarray(Image.open(df_dataset['path'][0])).shape\n",
    "print('Image shape:', img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stratified train/test/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset['path']\n",
    "y = df_dataset['cell_type_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a seed (random_state) for reproducibility and deterministic train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.111, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\n",
    "    'path': X_train,\n",
    "    'cell_type_idx': y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(path):\n",
    "    return np.asarray(Image.open(path), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_file(X_path, y):\n",
    "    \"\"\"\n",
    "    X_path: (pandas series) contains the file paths to the images\n",
    "    y: (pandas series of type int) the target label\n",
    "    \n",
    "    return a pair of numpy arrays representing (features, target)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X_path.apply(convert_image_to_array)\n",
    "    X /= 255.\n",
    "    X = X.values\n",
    "    X = list(X)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    y = y.map(lambda y: to_categorical(y, num_classes=len(CLASS_LABELS)))\n",
    "    y = y.values\n",
    "    y = list(y)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(path, model):\n",
    "    x = convert_image_to_array(path=path)\n",
    "    x /= 255.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = create_model_file(X_path=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the model\n",
    "\n",
    "Here we import a fitted model and we will select the layers on which the localization will be based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_DN201_w_ClssWgt_06-0.5685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=f'../serialized_models/{MODEL_NAME}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the values for `last_conv_layer_name` and `classifier_layer_names`, use `model.summary()` to see the names of all layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'block14_sepconv2'\n",
    "classifier_layer_names = ['local_avg_pool', 'flatten', 'prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a single image from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting examples: 2, 13, 14, 16, 728, 738, 740, 742\n",
    "SAMPLE_NUM = 16\n",
    "SAMPLE_PATH = X_test[SAMPLE_NUM]\n",
    "img = np.asarray(Image.open(SAMPLE_PATH))\n",
    "\n",
    "label_int = y_test[SAMPLE_NUM]\n",
    "label_abbreviation = CLASS_LABELS[label_int]\n",
    "print(f'The image contains {lesion_type_dict[label_abbreviation]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at manually uploaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/my_images/IMG_6065.jpg'\n",
    "img = np.asarray(Image.open(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.expand_dims(img, axis=0)\n",
    "img = tf.image.resize_with_crop_or_pad(image=img, target_height=450, target_width=600)\n",
    "img = img.numpy()\n",
    "img = img.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.astype(np.float32)\n",
    "img /= 255.\n",
    "img = np.expand_dims(img, axis=0)\n",
    "preds = model.predict(img)\n",
    "pred_class = np.argmax(preds)\n",
    "pred_label = lesion_type_dict[CLASS_LABELS[pred_class]]\n",
    "pred_prob = np.max(preds)\n",
    "print(f\"Predicted class: {pred_label} \\n Probability: {pred_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Grad-CAM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path):\n",
    "    # `img` is a PIL image of size 450x600\n",
    "    img = convert_image_to_array(path=img_path)\n",
    "    # `array` is a float32 Numpy array of shape (450, 600, 3)\n",
    "    array = img/255.\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First look at active regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image\n",
    "img_array = get_img_array(img_path=SAMPLE_PATH)\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "pred_class = np.argmax(preds)\n",
    "pred_label = lesion_type_dict[CLASS_LABELS[pred_class]]\n",
    "pred_prob = np.max(preds)\n",
    "print(f\"Predicted class: {pred_label} \\n Probability: {pred_prob} \\n Actual Class: {lesion_type_dict[label_abbreviation]}\")\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    ")\n",
    "\n",
    "# Display heatmap\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlay the heatmap on the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the original image\n",
    "img = np.asarray(Image.open(SAMPLE_PATH))\n",
    "\n",
    "# We rescale heatmap to a range 0-255\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "# We use jet colormap to colorize heatmap\n",
    "jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "# We use RGB values of the colormap\n",
    "jet_colors = jet(np.arange(256))[:, :3]\n",
    "jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "# We create an image with RGB colorized heatmap\n",
    "jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "# Superimpose the heatmap on original image\n",
    "superimposed_img = jet_heatmap * 0.2 + img\n",
    "superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "# Save the superimposed image\n",
    "save_path = \"skin.jpg\"\n",
    "superimposed_img.save(save_path)\n",
    "\n",
    "# Display Grad CAM along with original image\n",
    "fig, axs = plt.subplots(1, 2, figsize = (10, 5))\n",
    "\n",
    "axs[0].imshow(np.asarray(Image.open(save_path)))\n",
    "axs[0].axis('off')\n",
    "axs[0].set_aspect('auto')\n",
    "axs[1].imshow(np.asarray(Image.open(SAMPLE_PATH)))\n",
    "axs[1].axis('off')\n",
    "axs[1].set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (450, 600, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(img_input, top_pred_idx):\n",
    "    \"\"\"Computes the gradients of outputs w.r.t input image.\n",
    "\n",
    "    Args:\n",
    "        img_input: 4D image tensor\n",
    "        top_pred_idx: Predicted label for the input image\n",
    "\n",
    "    Returns:\n",
    "        Gradients of the predictions w.r.t img_input\n",
    "    \"\"\"\n",
    "    images = tf.cast(img_input, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        preds = model(images)\n",
    "        top_class = preds[:, top_pred_idx]\n",
    "\n",
    "    grads = tape.gradient(top_class, images)\n",
    "    return grads\n",
    "\n",
    "\n",
    "def get_integrated_gradients(img_input, top_pred_idx, baseline=None, num_steps=50):\n",
    "    \"\"\"Computes Integrated Gradients for a predicted label.\n",
    "\n",
    "    Args:\n",
    "        img_input (ndarray): Original image\n",
    "        top_pred_idx: Predicted label for the input image\n",
    "        baseline (ndarray): The baseline image to start with for interpolation\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "\n",
    "    Returns:\n",
    "        Integrated gradients w.r.t input image\n",
    "    \"\"\"\n",
    "    # If baseline is not provided, start with a black image\n",
    "    # having same size as the input image.\n",
    "    if baseline is None:\n",
    "        baseline = np.zeros(img_size).astype(np.float32)\n",
    "    else:\n",
    "        baseline = baseline.astype(np.float32)\n",
    "\n",
    "    # 1. Do interpolation.\n",
    "    img_input = img_input.astype(np.float32)\n",
    "    interpolated_image = [\n",
    "        baseline + (step / num_steps) * (img_input - baseline)\n",
    "        for step in range(num_steps + 1)\n",
    "    ]\n",
    "    interpolated_image = np.array(interpolated_image).astype(np.float32)\n",
    "\n",
    "    # 2. Preprocess the interpolated images\n",
    "    interpolated_image /= 255.\n",
    "\n",
    "    # 3. Get the gradients\n",
    "    grads = []\n",
    "    for i, img in enumerate(interpolated_image):\n",
    "        img = tf.expand_dims(img, axis=0)\n",
    "        grad = get_gradients(img, top_pred_idx=top_pred_idx)\n",
    "        grads.append(grad[0])\n",
    "    grads = tf.convert_to_tensor(grads, dtype=tf.float32)\n",
    "\n",
    "    # 4. Approximate the integral usiing the trapezoidal rule\n",
    "    grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "    avg_grads = tf.reduce_mean(grads, axis=0)\n",
    "\n",
    "    # 5. Calculate integrated gradients and return\n",
    "    integrated_grads = (img_input - baseline) * avg_grads\n",
    "    return integrated_grads\n",
    "\n",
    "\n",
    "def random_baseline_integrated_gradients(\n",
    "    img_input, top_pred_idx, num_steps=50, num_runs=2\n",
    "):\n",
    "    \"\"\"Generates a number of random baseline images.\n",
    "\n",
    "    Args:\n",
    "        img_input (ndarray): 3D image\n",
    "        top_pred_idx: Predicted label for the input image\n",
    "        num_steps: Number of interpolation steps between the baseline\n",
    "            and the input used in the computation of integrated gradients. These\n",
    "            steps along determine the integral approximation error. By default,\n",
    "            num_steps is set to 50.\n",
    "        num_runs: number of baseline images to generate\n",
    "\n",
    "    Returns:\n",
    "        Averaged integrated gradients for `num_runs` baseline images\n",
    "    \"\"\"\n",
    "    # 1. List to keep track of Integrated Gradients (IG) for all the images\n",
    "    integrated_grads = []\n",
    "\n",
    "    # 2. Get the integrated gradients for all the baselines\n",
    "    for run in range(num_runs):\n",
    "        baseline = np.random.random(img_size) * 255\n",
    "        igrads = get_integrated_gradients(\n",
    "            img_input=img_input,\n",
    "            top_pred_idx=top_pred_idx,\n",
    "            baseline=baseline,\n",
    "            num_steps=num_steps,\n",
    "        )\n",
    "        integrated_grads.append(igrads)\n",
    "\n",
    "    # 3. Return the average integrated gradients for the image\n",
    "    integrated_grads = tf.convert_to_tensor(integrated_grads)\n",
    "    return tf.reduce_mean(integrated_grads, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper class for visualizing gradients and integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradVisualizer:\n",
    "    \"\"\"Plot gradients of the outputs w.r.t an input image.\"\"\"\n",
    "\n",
    "    def __init__(self, positive_channel=None, negative_channel=None):\n",
    "        if positive_channel is None:\n",
    "            self.positive_channel = [0, 255, 0]\n",
    "        else:\n",
    "            self.positive_channel = positive_channel\n",
    "\n",
    "        if negative_channel is None:\n",
    "            self.negative_channel = [255, 0, 0]\n",
    "        else:\n",
    "            self.negative_channel = negative_channel\n",
    "\n",
    "    def apply_polarity(self, attributions, polarity):\n",
    "        if polarity == \"positive\":\n",
    "            return np.clip(attributions, 0, 1)\n",
    "        else:\n",
    "            return np.clip(attributions, -1, 0)\n",
    "\n",
    "    def apply_linear_transformation(\n",
    "        self,\n",
    "        attributions,\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=70.0,\n",
    "        lower_end=0.2,\n",
    "    ):\n",
    "        # 1. Get the thresholds\n",
    "        m = self.get_thresholded_attributions(\n",
    "            attributions, percentage=100 - clip_above_percentile\n",
    "        )\n",
    "        e = self.get_thresholded_attributions(\n",
    "            attributions, percentage=100 - clip_below_percentile\n",
    "        )\n",
    "\n",
    "        # 2. Transform the attributions by a linear function f(x) = a*x + b such that\n",
    "        # f(m) = 1.0 and f(e) = lower_end\n",
    "        transformed_attributions = (1 - lower_end) * (np.abs(attributions) - e) / (\n",
    "            m - e\n",
    "        ) + lower_end\n",
    "\n",
    "        # 3. Make sure that the sign of transformed attributions is the same as original attributions\n",
    "        transformed_attributions *= np.sign(attributions)\n",
    "\n",
    "        # 4. Only keep values that are bigger than the lower_end\n",
    "        transformed_attributions *= transformed_attributions >= lower_end\n",
    "\n",
    "        # 5. Clip values and return\n",
    "        transformed_attributions = np.clip(transformed_attributions, 0.0, 1.0)\n",
    "        return transformed_attributions\n",
    "\n",
    "    def get_thresholded_attributions(self, attributions, percentage):\n",
    "        if percentage == 100.0:\n",
    "            return np.min(attributions)\n",
    "\n",
    "        # 1. Flatten the attributions\n",
    "        flatten_attr = attributions.flatten()\n",
    "\n",
    "        # 2. Get the sum of the attributions\n",
    "        total = np.sum(flatten_attr)\n",
    "\n",
    "        # 3. Sort the attributions from largest to smallest.\n",
    "        sorted_attributions = np.sort(np.abs(flatten_attr))[::-1]\n",
    "\n",
    "        # 4. Calculate the percentage of the total sum that each attribution\n",
    "        # and the values about it contribute.\n",
    "        cum_sum = 100.0 * np.cumsum(sorted_attributions) / total\n",
    "\n",
    "        # 5. Threshold the attributions by the percentage\n",
    "        indices_to_consider = np.where(cum_sum >= percentage)[0][0]\n",
    "\n",
    "        # 6. Select the desired attributions and return\n",
    "        attributions = sorted_attributions[indices_to_consider]\n",
    "        return attributions\n",
    "\n",
    "    def binarize(self, attributions, threshold=0.001):\n",
    "        return attributions > threshold\n",
    "\n",
    "    def morphological_cleanup_fn(self, attributions, structure=np.ones((4, 4))):\n",
    "        closed = ndimage.grey_closing(attributions, structure=structure)\n",
    "        opened = ndimage.grey_opening(closed, structure=structure)\n",
    "        return opened\n",
    "\n",
    "    def draw_outlines(\n",
    "        self, attributions, percentage=90, connected_component_structure=np.ones((3, 3))\n",
    "    ):\n",
    "        # 1. Binarize the attributions.\n",
    "        attributions = self.binarize(attributions)\n",
    "\n",
    "        # 2. Fill the gaps\n",
    "        attributions = ndimage.binary_fill_holes(attributions)\n",
    "\n",
    "        # 3. Compute connected components\n",
    "        connected_components, num_comp = ndimage.measurements.label(\n",
    "            attributions, structure=connected_component_structure\n",
    "        )\n",
    "\n",
    "        # 4. Sum up the attributions for each component\n",
    "        total = np.sum(attributions[connected_components > 0])\n",
    "        component_sums = []\n",
    "        for comp in range(1, num_comp + 1):\n",
    "            mask = connected_components == comp\n",
    "            component_sum = np.sum(attributions[mask])\n",
    "            component_sums.append((component_sum, mask))\n",
    "\n",
    "        # 5. Compute the percentage of top components to keep\n",
    "        sorted_sums_and_masks = sorted(component_sums, key=lambda x: x[0], reverse=True)\n",
    "        sorted_sums = list(zip(*sorted_sums_and_masks))[0]\n",
    "        cumulative_sorted_sums = np.cumsum(sorted_sums)\n",
    "        cutoff_threshold = percentage * total / 100\n",
    "        cutoff_idx = np.where(cumulative_sorted_sums >= cutoff_threshold)[0][0]\n",
    "        if cutoff_idx > 2:\n",
    "            cutoff_idx = 2\n",
    "\n",
    "        # 6. Set the values for the kept components\n",
    "        border_mask = np.zeros_like(attributions)\n",
    "        for i in range(cutoff_idx + 1):\n",
    "            border_mask[sorted_sums_and_masks[i][1]] = 1\n",
    "\n",
    "        # 7. Make the mask hollow and show only the border\n",
    "        eroded_mask = ndimage.binary_erosion(border_mask, iterations=1)\n",
    "        border_mask[eroded_mask] = 0\n",
    "\n",
    "        # 8. Return the outlined mask\n",
    "        return border_mask\n",
    "\n",
    "    def process_grads(\n",
    "        self,\n",
    "        image,\n",
    "        attributions,\n",
    "        polarity=\"positive\",\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=0,\n",
    "        morphological_cleanup=False,\n",
    "        structure=np.ones((3, 3)),\n",
    "        outlines=False,\n",
    "        outlines_component_percentage=90,\n",
    "        overlay=True,\n",
    "    ):\n",
    "        if polarity not in [\"positive\", \"negative\"]:\n",
    "            raise ValueError(\n",
    "                f\"\"\" Allowed polarity values: 'positive' or 'negative'\n",
    "                                    but provided {polarity}\"\"\"\n",
    "            )\n",
    "        if clip_above_percentile < 0 or clip_above_percentile > 100:\n",
    "            raise ValueError(\"clip_above_percentile must be in [0, 100]\")\n",
    "\n",
    "        if clip_below_percentile < 0 or clip_below_percentile > 100:\n",
    "            raise ValueError(\"clip_below_percentile must be in [0, 100]\")\n",
    "\n",
    "        # 1. Apply polarity\n",
    "        if polarity == \"positive\":\n",
    "            attributions = self.apply_polarity(attributions, polarity=polarity)\n",
    "            channel = self.positive_channel\n",
    "        else:\n",
    "            attributions = self.apply_polarity(attributions, polarity=polarity)\n",
    "            attributions = np.abs(attributions)\n",
    "            channel = self.negative_channel\n",
    "\n",
    "        # 2. Take average over the channels\n",
    "        attributions = np.average(attributions, axis=2)\n",
    "\n",
    "        # 3. Apply linear transformation to the attributions\n",
    "        attributions = self.apply_linear_transformation(\n",
    "            attributions,\n",
    "            clip_above_percentile=clip_above_percentile,\n",
    "            clip_below_percentile=clip_below_percentile,\n",
    "            lower_end=0.0,\n",
    "        )\n",
    "\n",
    "        # 4. Cleanup\n",
    "        if morphological_cleanup:\n",
    "            attributions = self.morphological_cleanup_fn(\n",
    "                attributions, structure=structure\n",
    "            )\n",
    "        # 5. Draw the outlines\n",
    "        if outlines:\n",
    "            attributions = self.draw_outlines(\n",
    "                attributions, percentage=outlines_component_percentage\n",
    "            )\n",
    "\n",
    "        # 6. Expand the channel axis and convert to RGB\n",
    "        attributions = np.expand_dims(attributions, 2) * channel\n",
    "\n",
    "        # 7.Superimpose on the original image\n",
    "        if overlay:\n",
    "            attributions = np.clip((attributions * 0.8 + image), 0, 255)\n",
    "        return attributions\n",
    "\n",
    "    def visualize(\n",
    "        self,\n",
    "        image,\n",
    "        gradients,\n",
    "        integrated_gradients,\n",
    "        polarity=\"positive\",\n",
    "        clip_above_percentile=99.9,\n",
    "        clip_below_percentile=0,\n",
    "        morphological_cleanup=False,\n",
    "        structure=np.ones((3, 3)),\n",
    "        outlines=False,\n",
    "        outlines_component_percentage=90,\n",
    "        overlay=True,\n",
    "        figsize=(15, 8),\n",
    "    ):\n",
    "        # 1. Make two copies of the original image\n",
    "        img1 = np.copy(image)\n",
    "        img2 = np.copy(image)\n",
    "\n",
    "        # 2. Process the normal gradients\n",
    "        grads_attr = self.process_grads(\n",
    "            image=img1,\n",
    "            attributions=gradients,\n",
    "            polarity=polarity,\n",
    "            clip_above_percentile=clip_above_percentile,\n",
    "            clip_below_percentile=clip_below_percentile,\n",
    "            morphological_cleanup=morphological_cleanup,\n",
    "            structure=structure,\n",
    "            outlines=outlines,\n",
    "            outlines_component_percentage=outlines_component_percentage,\n",
    "            overlay=overlay,\n",
    "        )\n",
    "\n",
    "        # 3. Process the integrated gradients\n",
    "        igrads_attr = self.process_grads(\n",
    "            image=img2,\n",
    "            attributions=integrated_gradients,\n",
    "            polarity=polarity,\n",
    "            clip_above_percentile=clip_above_percentile,\n",
    "            clip_below_percentile=clip_below_percentile,\n",
    "            morphological_cleanup=morphological_cleanup,\n",
    "            structure=structure,\n",
    "            outlines=outlines,\n",
    "            outlines_component_percentage=outlines_component_percentage,\n",
    "            overlay=overlay,\n",
    "        )\n",
    "\n",
    "        _, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(grads_attr.astype(np.uint8))\n",
    "        ax[2].imshow(igrads_attr.astype(np.uint8))\n",
    "\n",
    "        ax[0].set_title(\"Input\")\n",
    "        ax[1].set_title(\"Normal gradients\")\n",
    "        ax[2].set_title(\"Integrated gradients\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the image to numpy array\n",
    "img = convert_image_to_array(path=SAMPLE_PATH)\n",
    "\n",
    "# 2. Keep a copy of the original image\n",
    "orig_img = np.copy(img[0]).astype(np.uint8)\n",
    "\n",
    "# 3. Preprocess the image\n",
    "img_processed = tf.cast(get_img_array(img_path=SAMPLE_PATH), dtype=tf.float32)\n",
    "\n",
    "# 4. Get model predictions\n",
    "preds = model.predict(img_processed)\n",
    "top_pred_idx = np.argmax(preds)\n",
    "pred_label = lesion_type_dict[CLASS_LABELS[top_pred_idx]]\n",
    "pred_prob = np.max(preds)\n",
    "print(f\"Predicted class: {pred_label} \\n Probability: {pred_prob} \\n Actual Class: {lesion_type_dict[label_abbreviation]}\")\n",
    "\n",
    "# 5. Get the gradients of the last layer for the predicted label\n",
    "grads = get_gradients(img_processed, top_pred_idx=top_pred_idx)\n",
    "\n",
    "# 6. Get the integrated gradients\n",
    "igrads = random_baseline_integrated_gradients(\n",
    "    np.copy(orig_img), top_pred_idx=top_pred_idx, num_steps=50, num_runs=2\n",
    ")\n",
    "\n",
    "# 7. Process the gradients and plot\n",
    "vis = GradVisualizer()\n",
    "vis.visualize(\n",
    "    image=orig_img,\n",
    "    gradients=grads[0].numpy(),\n",
    "    integrated_gradients=igrads.numpy(),\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    ")\n",
    "\n",
    "vis.visualize(\n",
    "    image=orig_img,\n",
    "    gradients=grads[0].numpy(),\n",
    "    integrated_gradients=igrads.numpy(),\n",
    "    clip_above_percentile=95,\n",
    "    clip_below_percentile=28,\n",
    "    morphological_cleanup=True,\n",
    "    outlines=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
