{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "\n",
    "This notebook will allow us to compare trained models on the test set. The analysis and metrics are standard for classification models, and the code is based on week 2 of Coursera's [AI for Medical Diagnosis](https://www.coursera.org/learn/ai-for-medical-diagnosis) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Move the functions out of this notebook and put them in the utils folder\n",
    "- Add confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from utils.metrics_utils import get_performance_metrics, print_confidence_intervals, get_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a dictionary of images for our dataset and create a lookup table for readable names for our classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.join('..', 'data')\n",
    "\n",
    "# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\n",
    "\n",
    "image_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(base_dir, '*', '*.jpg'))}\n",
    "\n",
    "# This dictionary is useful for displaying more human-friendly labels later on\n",
    "\n",
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Benign keratosis-like lesions',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(image_path_dict)} images in our dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will read and process the data. This will help later with creating labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df = pd.read_csv(os.path.join(base_dir, 'datasets_54339_104884_HAM10000_metadata.csv'))\n",
    "\n",
    "# Creating New Columns for better readability\n",
    "\n",
    "skin_df['path'] = skin_df['image_id'].map(image_path_dict.get)\n",
    "skin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get) \n",
    "skin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "Look for duplicate images from patients and make sure datasets are stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = skin_df.groupby('lesion_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original dataset had {skin_df.shape[0]} records, there are {df.shape[0]} unique lesions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Test, and Val Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO Use TF Records and tf dataset to store training dataset as an alternative to data generator below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are numerous images taken for some patients, therefore we will choose a single image from each patient. Then we will take a stratified sample across our target variable in order to create our train test and validation directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a dataframe containing a single image from each patient. Note that we could also try including these duplicates, just making sure that when we split our dataset we keep patients in a single train, test, or val set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed (random_state) for reproducibility and deterministic train/val/test sets\n",
    "df_dataset = skin_df.sample(frac=1, random_state=123).drop_duplicates(subset='lesion_id').copy()\n",
    "df_dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = [\n",
    "    'nv' ,\n",
    "    'mel', \n",
    "    'bkl', \n",
    "    'bcc',\n",
    "    'akiec',\n",
    "    'vasc',\n",
    "    'df',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = np.asarray(Image.open(df_dataset['path'][0])).shape\n",
    "print('Image shape:', img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stratified train/test/val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset['path']\n",
    "y = df_dataset['cell_type_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a seed (random_state) for reproducibility and deterministic train/val/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.111, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\n",
    "    'path': X_train,\n",
    "    'cell_type_idx': y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(path):\n",
    "    return np.asarray(Image.open(path), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_file(X_path, y):\n",
    "    \"\"\"\n",
    "    X_path: (pandas series) contains the file paths to the images\n",
    "    y: (pandas series of type int) the target label\n",
    "    \n",
    "    return a pair of numpy arrays representing (features, target)\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X_path.apply(convert_image_to_array)\n",
    "    X /= 255.\n",
    "    X = X.values\n",
    "    X = list(X)\n",
    "    X = np.array(X)\n",
    "    \n",
    "    y = y.map(lambda y: to_categorical(y, num_classes=len(CLASS_LABELS)))\n",
    "    y = y.values\n",
    "    y = list(y)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(path, model):\n",
    "    x = convert_image_to_array(path=path)\n",
    "    x /= 255.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = create_model_file(X_path=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the model\n",
    "\n",
    "Here we import a fitted model and we will select the layers on which the localization will be based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'model_DN201_w_ClssWgt_06-0.5685'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(filepath=f'../serialized_models/{MODEL_NAME}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUM = 0\n",
    "SAMPLE_PATH = X_test[SAMPLE_NUM]\n",
    "img = np.asarray(Image.open(SAMPLE_PATH))\n",
    "\n",
    "label_int = y_test[SAMPLE_NUM]\n",
    "label_abbreviation = CLASS_LABELS[label_int]\n",
    "print(f'The image contains {lesion_type_dict[label_abbreviation]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax.set_aspect('auto')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict(path=SAMPLE_PATH, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_LABELS = [l + \"_pred\" for l in CLASS_LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_preds = pd.DataFrame(0, index=np.arange(len(X_test)), columns=PRED_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(X_test):\n",
    "    all_model_preds.iloc[i, :] = model_predict(path=path, model=model)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.get_dummies(data=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rename = {i:label for i, label in enumerate(CLASS_LABELS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.rename(columns=col_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, all_model_preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = results[CLASS_LABELS].values\n",
    "pred = results[PRED_LABELS].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x = CLASS_LABELS, height= y.sum(axis=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the evaluatin metrics we will use can be computed from the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Below these are defined along with a small unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count true positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        TP (int): true positives\n",
    "    \"\"\"\n",
    "    TP = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "\n",
    "    # compute TP\n",
    "    TP = np.sum((y == 1) & (thresholded_preds == 1))\n",
    "    \n",
    "    return TP\n",
    "\n",
    "def true_negatives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count true negatives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        TN (int): true negatives\n",
    "    \"\"\"\n",
    "    TN = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    \n",
    "    # compute TN\n",
    "    TN = np.sum((y == 0) & (thresholded_preds == 0))\n",
    "    \n",
    "    return TN\n",
    "\n",
    "def false_positives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count false positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        FP (int): false positives\n",
    "    \"\"\"\n",
    "    FP = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "\n",
    "    # compute FP\n",
    "    FP = np.sum((y == 0) & (thresholded_preds == 1))\n",
    "    \n",
    "    return FP\n",
    "\n",
    "def false_negatives(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Count false positives.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        FN (int): false negatives\n",
    "    \"\"\"\n",
    "    FN = 0\n",
    "    \n",
    "    # get thresholded predictions\n",
    "    thresholded_preds = pred >= th\n",
    "    \n",
    "    # compute FN\n",
    "    FN = np.sum((y == 1) & (thresholded_preds == 0))\n",
    "    \n",
    "    return FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'y_test': [1,1,0,0,0,0,0,0,0,1,1,1,1,1],\n",
    "                   'preds_test': [0.8,0.7,0.4,0.3,0.2,0.5,0.6,0.7,0.8,0.1,0.2,0.3,0.4,0],\n",
    "                   'category': ['TP','TP','TN','TN','TN','FP','FP','FP','FP','FN','FN','FN','FN','FN']\n",
    "                  })\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = np.array([1, 0, 0, 1, 1])\n",
    "y_test = df['y_test']\n",
    "\n",
    "#preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "preds_test = df['preds_test']\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"\"\"Our functions calcualted: \n",
    "TP: {true_positives(y_test, preds_test, threshold)}\n",
    "TN: {true_negatives(y_test, preds_test, threshold)}\n",
    "FP: {false_positives(y_test, preds_test, threshold)}\n",
    "FN: {false_negatives(y_test, preds_test, threshold)}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Expected results\")\n",
    "print(f\"There are {sum(df['category'] == 'TP')} TP\")\n",
    "print(f\"There are {sum(df['category'] == 'TN')} TN\")\n",
    "print(f\"There are {sum(df['category'] == 'FP')} FP\")\n",
    "print(f\"There are {sum(df['category'] == 'FN')} FN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        accuracy (float): accuracy of predictions at threshold\n",
    "    \"\"\"\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    # get TP, FP, TN, FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    # Compute accuracy using TP, FP, TN, FN\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print('test labels: {y_test}')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\")\n",
    "\n",
    "print(f\"computed accuracy: {get_accuracy(y_test, preds_test, threshold)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "```Python\n",
    "test labels: {y_test}\n",
    "test predictions: [0.8 0.8 0.4 0.6 0.3]\n",
    "threshold: 0.5\n",
    "computed accuracy: 0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prevalence(y):\n",
    "    \"\"\"\n",
    "    Compute accuracy of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "    Returns:\n",
    "        prevalence (float): prevalence of positive cases\n",
    "    \"\"\"\n",
    "    prevalence = 0.0\n",
    "    \n",
    "    prevalence = np.sum(y) / len(y)\n",
    "    \n",
    "    return prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\\n\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "print(f'test labels: {y_test}')\n",
    "\n",
    "print(f\"computed prevalence: {get_prevalence(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy, prevalence=get_prevalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensitivity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute sensitivity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n",
    "    \"\"\"\n",
    "    sensitivity = 0.0\n",
    "    \n",
    "    # get TP and FN using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    # use TP and FN to compute sensitivity\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    \n",
    "    return sensitivity\n",
    "\n",
    "def get_specificity(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute specificity of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        specificity (float): probability that the test outputs negative given that the case is actually negative\n",
    "    \"\"\"\n",
    "    specificity = 0.0\n",
    "    \n",
    "    # get TN and FP using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "    \n",
    "    # use TN and FP to compute specificity \n",
    "    specificity = TN / (TN + FP)\n",
    "    \n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print(f'test labels: {y_test}\\n')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}\\n')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"computed sensitivity: {get_sensitivity(y_test, preds_test, threshold):.2f}\")\n",
    "print(f\"computed specificity: {get_specificity(y_test, preds_test, threshold):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "```Python\n",
    "Test case\n",
    "test labels: [1 0 0 1 1]\n",
    "\n",
    "test predictions: [0.8 0.8 0.4 0.6 0.3]\n",
    "\n",
    "threshold: 0.5\n",
    "\n",
    "computed sensitivity: 0.67\n",
    "computed specificity: 0.50\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ppv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute PPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        PPV (float): positive predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    PPV = 0.0\n",
    "    \n",
    "    # get TP and FP using our previously defined functions\n",
    "    TP = true_positives(y, pred, th)\n",
    "    FP = false_positives(y, pred, th)\n",
    "\n",
    "    # use TP and FP to compute PPV\n",
    "    PPV = TP / (TP + FP)\n",
    "    \n",
    "    return PPV\n",
    "\n",
    "def get_npv(y, pred, th=0.5):\n",
    "    \"\"\"\n",
    "    Compute NPV of predictions at threshold.\n",
    "\n",
    "    Args:\n",
    "        y (np.array): ground truth, size (n_examples)\n",
    "        pred (np.array): model output, size (n_examples)\n",
    "        th (float): cutoff value for positive prediction from model\n",
    "    Returns:\n",
    "        NPV (float): negative predictive value of predictions at threshold\n",
    "    \"\"\"\n",
    "    NPV = 0.0\n",
    "    \n",
    "    # get TN and FN using our previously defined functions\n",
    "    TN = true_negatives(y, pred, th)\n",
    "    FN = false_negatives(y, pred, th)\n",
    "\n",
    "    # use TN and FN to compute NPV\n",
    "    NPV = TN / (TN + FN)\n",
    "    \n",
    "    return NPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Test case:\\n\")\n",
    "\n",
    "y_test = np.array([1, 0, 0, 1, 1])\n",
    "print(f'test labels: {y_test}')\n",
    "\n",
    "preds_test = np.array([0.8, 0.8, 0.4, 0.6, 0.3])\n",
    "print(f'test predictions: {preds_test}\\n')\n",
    "\n",
    "threshold = 0.5\n",
    "print(f\"threshold: {threshold}\\n\")\n",
    "\n",
    "print(f\"computed ppv: {get_ppv(y_test, preds_test, threshold):.2f}\")\n",
    "print(f\"computed npv: {get_npv(y_test, preds_test, threshold):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "```Python\n",
    "Test case:\n",
    "\n",
    "test labels: [1 0 0 1 1]\n",
    "test predictions: [0.8 0.8 0.4 0.6 0.3]\n",
    "\n",
    "threshold: 0.5\n",
    "\n",
    "computed ppv: 0.67\n",
    "computed npv: 0.50\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_curve(y, pred, CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n",
    "    statistics = np.zeros((len(classes), bootstraps))\n",
    "\n",
    "    for c in range(len(classes)):\n",
    "        df = pd.DataFrame(columns=['y', 'pred'])\n",
    "        df.loc[:, 'y'] = y[:, c]\n",
    "        df.loc[:, 'pred'] = pred[:, c]\n",
    "        # get positive examples for stratified sampling\n",
    "        df_pos = df[df.y == 1]\n",
    "        df_neg = df[df.y == 0]\n",
    "        prevalence = len(df_pos) / len(df)\n",
    "        for i in range(bootstraps):\n",
    "            # stratified sampling of positive and negative examples\n",
    "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
    "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
    "\n",
    "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
    "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
    "            score = roc_auc_score(y_sample, pred_sample)\n",
    "            statistics[c][i] = score\n",
    "    return statistics\n",
    "\n",
    "statistics = bootstrap_auc(y, pred, CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confidence_intervals(CLASS_LABELS, statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_curve(y, pred, CLASS_LABELS, curve='prc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "def plot_calibration_curve(y, pred):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i in range(len(CLASS_LABELS)):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "        plt.plot(mean_predicted_value, fraction_of_positives, marker='.')\n",
    "        plt.xlabel(\"Predicted Value\")\n",
    "        plt.ylabel(\"Fraction of Positives\")\n",
    "        plt.title(CLASS_LABELS[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "model_performance_df = get_performance_metrics(y, pred, CLASS_LABELS, acc=get_accuracy, prevalence=get_prevalence, \n",
    "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the test set results\n",
    "\n",
    "Save the results along with the model name so that we can compare various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df.rename(index=lesion_type_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_df.to_csv(f'test_set_performance/test_metrics_{MODEL_NAME}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
